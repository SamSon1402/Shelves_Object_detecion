{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import kagglehub\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Download dataset if not already downloaded\n",
        "def download_dataset():\n",
        "    print(\"Downloading supermarket shelves dataset...\")\n",
        "    path = kagglehub.dataset_download(\"humansintheloop/supermarket-shelves-dataset\")\n",
        "    print(f\"Dataset downloaded to: {path}\")\n",
        "\n",
        "    # Explore the directory structure\n",
        "    print(\"\\nExploring dataset directory structure:\")\n",
        "    found_image_dir = None\n",
        "    found_anno_dir = None\n",
        "\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        # Skip .git directories\n",
        "        if '.git' in root:\n",
        "            continue\n",
        "\n",
        "        level = root.replace(path, '').count(os.sep)\n",
        "        indent = ' ' * 4 * level\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "\n",
        "        # Look for likely image and annotation directories\n",
        "        image_exts = ['.jpg', '.jpeg', '.png']\n",
        "        json_exts = ['.json']\n",
        "\n",
        "        # Count image and JSON files in this directory\n",
        "        img_count = sum(1 for f in files if os.path.splitext(f)[1].lower() in image_exts)\n",
        "        json_count = sum(1 for f in files if os.path.splitext(f)[1].lower() in json_exts)\n",
        "\n",
        "        # Print file counts\n",
        "        if img_count > 0:\n",
        "            print(f\"{indent}    (Found {img_count} image files)\")\n",
        "            # Record this as a potential image directory if it has the most images so far\n",
        "            if found_image_dir is None or img_count > found_image_dir[1]:\n",
        "                found_image_dir = (root, img_count)\n",
        "\n",
        "        if json_count > 0:\n",
        "            print(f\"{indent}    (Found {json_count} JSON files)\")\n",
        "            # Record this as a potential annotation directory if it has the most JSONs so far\n",
        "            if found_anno_dir is None or json_count > found_anno_dir[1]:\n",
        "                found_anno_dir = (root, json_count)\n",
        "\n",
        "    # Print the discovered directories\n",
        "    if found_image_dir:\n",
        "        print(f\"\\nFound likely image directory: {found_image_dir[0]} ({found_image_dir[1]} images)\")\n",
        "    if found_anno_dir:\n",
        "        print(f\"Found likely annotation directory: {found_anno_dir[0]} ({found_anno_dir[1]} JSON files)\")\n",
        "\n",
        "    # Create a dataset info dictionary to return\n",
        "    dataset_info = {\n",
        "        'base_path': path,\n",
        "        'image_dir': found_image_dir[0] if found_image_dir else None,\n",
        "        'anno_dir': found_anno_dir[0] if found_anno_dir else None,\n",
        "        'meta_dir': found_anno_dir[0] if found_anno_dir else path  # Assume meta.json is in the same dir as annotations\n",
        "    }\n",
        "\n",
        "    return dataset_info\n",
        "\n",
        "# Function to read meta.json and get class information\n",
        "def get_class_info(dataset_info):\n",
        "    # Look for meta.json in various locations\n",
        "    possible_meta_paths = [\n",
        "        os.path.join(dataset_info['meta_dir'], \"meta.json\"),\n",
        "        os.path.join(dataset_info['base_path'], \"meta.json\")\n",
        "    ]\n",
        "\n",
        "    # Also look in subdirectories of the base path\n",
        "    for root, _, files in os.walk(dataset_info['base_path']):\n",
        "        if \"meta.json\" in files:\n",
        "            possible_meta_paths.append(os.path.join(root, \"meta.json\"))\n",
        "\n",
        "    meta_path = None\n",
        "    meta_data = None\n",
        "\n",
        "    # Try each path until we find a valid meta.json\n",
        "    for path in possible_meta_paths:\n",
        "        if os.path.exists(path):\n",
        "            print(f\"Found meta.json at {path}\")\n",
        "            try:\n",
        "                with open(path, 'r') as f:\n",
        "                    meta_data = json.load(f)\n",
        "                meta_path = path\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading {path}: {e}\")\n",
        "\n",
        "    if not meta_data:\n",
        "        print(\"Could not find or parse meta.json, checking annotation files instead\")\n",
        "        return infer_classes_from_annotations(dataset_info)\n",
        "\n",
        "    # Extract classes - structure might vary, so let's try common patterns\n",
        "    classes = []\n",
        "    if \"classes\" in meta_data:\n",
        "        # Handle if classes are dictionaries with 'title' field\n",
        "        raw_classes = meta_data[\"classes\"]\n",
        "        if isinstance(raw_classes, list) and len(raw_classes) > 0 and isinstance(raw_classes[0], dict):\n",
        "            if \"title\" in raw_classes[0]:\n",
        "                classes = [cls[\"title\"] for cls in raw_classes]\n",
        "            elif \"name\" in raw_classes[0]:\n",
        "                classes = [cls[\"name\"] for cls in raw_classes]\n",
        "            else:\n",
        "                # If we can't find title or name, use the first string key we find\n",
        "                for key in raw_classes[0].keys():\n",
        "                    if isinstance(raw_classes[0][key], str):\n",
        "                        classes = [cls[key] for cls in raw_classes]\n",
        "                        break\n",
        "        else:\n",
        "            classes = raw_classes\n",
        "    elif \"categories\" in meta_data:\n",
        "        classes = [cat[\"name\"] for cat in meta_data[\"categories\"]]\n",
        "    elif \"labels\" in meta_data:\n",
        "        classes = meta_data[\"labels\"]\n",
        "\n",
        "    # If still empty, try to infer from an annotation file\n",
        "    if not classes:\n",
        "        print(\"Classes not found in meta.json, inferring from annotation file...\")\n",
        "        return infer_classes_from_annotations(dataset_info)\n",
        "\n",
        "    print(f\"Found {len(classes)} classes: {classes}\")\n",
        "    return classes\n",
        "\n",
        "# Infer classes from annotations as a fallback\n",
        "def infer_classes_from_annotations(dataset_info):\n",
        "    print(\"Attempting to infer classes from annotation files...\")\n",
        "    anno_dir = dataset_info['anno_dir']\n",
        "\n",
        "    if not anno_dir or not os.path.exists(anno_dir):\n",
        "        print(f\"Error: Annotation directory not found at {anno_dir}\")\n",
        "        return []\n",
        "\n",
        "    classes = set()\n",
        "\n",
        "    # Get all JSON files in the annotation directory\n",
        "    anno_files = [f for f in os.listdir(anno_dir) if f.endswith('.json')]\n",
        "\n",
        "    # Limit to the first 10 files to avoid long processing time\n",
        "    anno_files = anno_files[:10]\n",
        "\n",
        "    for anno_file in anno_files:\n",
        "        try:\n",
        "            with open(os.path.join(anno_dir, anno_file), 'r') as f:\n",
        "                anno_data = json.load(f)\n",
        "\n",
        "            # SuperviselyIO format\n",
        "            if \"objects\" in anno_data:\n",
        "                for obj in anno_data[\"objects\"]:\n",
        "                    if \"classTitle\" in obj:\n",
        "                        classes.add(obj[\"classTitle\"])\n",
        "\n",
        "            # LabelMe format\n",
        "            elif \"shapes\" in anno_data:\n",
        "                for shape in anno_data[\"shapes\"]:\n",
        "                    if \"label\" in shape:\n",
        "                        classes.add(shape[\"label\"])\n",
        "\n",
        "            # COCO format\n",
        "            elif \"annotations\" in anno_data:\n",
        "                if \"categories\" in anno_data:\n",
        "                    for cat in anno_data[\"categories\"]:\n",
        "                        if \"name\" in cat:\n",
        "                            classes.add(cat[\"name\"])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {anno_file}: {e}\")\n",
        "\n",
        "    classes = list(classes)\n",
        "    print(f\"Inferred {len(classes)} classes: {classes}\")\n",
        "    return classes\n",
        "\n",
        "# Convert COCO/JSON annotations to YOLO format\n",
        "def convert_annotations(dataset_info, classes):\n",
        "    images_path = dataset_info['image_dir']\n",
        "    annotations_path = dataset_info['anno_dir']\n",
        "\n",
        "    if not images_path or not os.path.exists(images_path):\n",
        "        raise FileNotFoundError(f\"Image directory not found at {images_path}\")\n",
        "\n",
        "    if not annotations_path or not os.path.exists(annotations_path):\n",
        "        raise FileNotFoundError(f\"Annotation directory not found at {annotations_path}\")\n",
        "\n",
        "    print(f\"Using image directory: {images_path}\")\n",
        "    print(f\"Using annotation directory: {annotations_path}\")\n",
        "\n",
        "    # Create output directories\n",
        "    output_base = \"yolo_dataset\"\n",
        "    os.makedirs(os.path.join(output_base, \"images\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_base, \"labels\"), exist_ok=True)\n",
        "\n",
        "    # Create class mapping\n",
        "    class_to_id = {cls_name: i for i, cls_name in enumerate(classes)}\n",
        "\n",
        "    # Store paths for all processed files\n",
        "    all_image_paths = []\n",
        "\n",
        "    print(\"Converting annotations to YOLO format...\")\n",
        "\n",
        "    # Get all image files\n",
        "    image_files = [f for f in os.listdir(images_path)\n",
        "                  if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "    # Get all annotation files\n",
        "    anno_files = [f for f in os.listdir(annotations_path)\n",
        "                 if f.lower().endswith('.json')]\n",
        "\n",
        "    print(f\"Found {len(image_files)} images and {len(anno_files)} annotation files\")\n",
        "\n",
        "    # Create a mapping from image names to annotation files\n",
        "    # This handles cases where the naming conventions might differ\n",
        "    image_to_anno = {}\n",
        "\n",
        "    for anno_file in anno_files:\n",
        "        # Try different matching strategies\n",
        "        base_name = os.path.splitext(anno_file)[0]\n",
        "\n",
        "        # Case 1: annotation is named exactly like the image with .json added\n",
        "        if base_name in image_files:\n",
        "            image_to_anno[base_name] = anno_file\n",
        "\n",
        "        # Case 2: annotation is named like the image without extension\n",
        "        for img_file in image_files:\n",
        "            img_base = os.path.splitext(img_file)[0]\n",
        "            if img_base == base_name:\n",
        "                image_to_anno[img_file] = anno_file\n",
        "                break\n",
        "\n",
        "    print(f\"Successfully matched {len(image_to_anno)} images with annotations\")\n",
        "\n",
        "    for img_file, anno_file in tqdm(image_to_anno.items()):\n",
        "        img_path = os.path.join(images_path, img_file)\n",
        "        anno_path = os.path.join(annotations_path, anno_file)\n",
        "\n",
        "        try:\n",
        "            with open(anno_path, 'r') as f:\n",
        "                anno_data = json.load(f)\n",
        "\n",
        "            # Get image dimensions - try to handle different annotation formats\n",
        "            img_width = img_height = None\n",
        "\n",
        "            # Try to get dimensions from the annotation file\n",
        "            if \"imageWidth\" in anno_data and \"imageHeight\" in anno_data:\n",
        "                img_width = anno_data[\"imageWidth\"]\n",
        "                img_height = anno_data[\"imageHeight\"]\n",
        "            elif \"width\" in anno_data and \"height\" in anno_data:\n",
        "                img_width = anno_data[\"width\"]\n",
        "                img_height = anno_data[\"height\"]\n",
        "            elif \"image\" in anno_data and isinstance(anno_data[\"image\"], dict):\n",
        "                img_width = anno_data[\"image\"].get(\"width\")\n",
        "                img_height = anno_data[\"image\"].get(\"height\")\n",
        "\n",
        "            # If dimensions not found, use pillow to get them\n",
        "            if not img_width or not img_height:\n",
        "                try:\n",
        "                    from PIL import Image\n",
        "                    with Image.open(img_path) as img:\n",
        "                        img_width, img_height = img.size\n",
        "                except Exception as e:\n",
        "                    print(f\"Error getting image dimensions for {img_file}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            # Extract annotations\n",
        "            yolo_annotations = []\n",
        "\n",
        "            # Handle SuperviselyIO format which seems to be used in this dataset\n",
        "            if \"objects\" in anno_data:\n",
        "                for obj in anno_data[\"objects\"]:\n",
        "                    # Get class title\n",
        "                    class_title = obj.get(\"classTitle\")\n",
        "\n",
        "                    # Skip if class not in our class list\n",
        "                    if class_title not in class_to_id:\n",
        "                        print(f\"Warning: Class '{class_title}' not in class list. Available classes: {classes}\")\n",
        "                        continue\n",
        "\n",
        "                    class_id = class_to_id[class_title]\n",
        "\n",
        "                    # Get bounding box points\n",
        "                    if \"points\" in obj:\n",
        "                        # Extract points - format can vary\n",
        "                        points = None\n",
        "\n",
        "                        # Handle different points formats\n",
        "                        if \"exterior\" in obj[\"points\"]:\n",
        "                            points = obj[\"points\"][\"exterior\"]\n",
        "                        elif isinstance(obj[\"points\"], list):\n",
        "                            points = obj[\"points\"]\n",
        "\n",
        "                        if points and len(points) >= 2:\n",
        "                            # For rectangle, find min/max points\n",
        "                            x_coords = [p[0] for p in points]\n",
        "                            y_coords = [p[1] for p in points]\n",
        "\n",
        "                            x1, y1 = min(x_coords), min(y_coords)\n",
        "                            x2, y2 = max(x_coords), max(y_coords)\n",
        "\n",
        "                            # Calculate width and height\n",
        "                            w = x2 - x1\n",
        "                            h = y2 - y1\n",
        "\n",
        "                            # Convert to YOLO format\n",
        "                            x_center = (x1 + w/2) / img_width\n",
        "                            y_center = (y1 + h/2) / img_height\n",
        "                            w_norm = w / img_width\n",
        "                            h_norm = h / img_height\n",
        "\n",
        "                            yolo_annotations.append(f\"{class_id} {x_center} {y_center} {w_norm} {h_norm}\")\n",
        "\n",
        "                    # Handle bitmap format by converting to bounding box\n",
        "                    elif \"bitmap\" in obj:\n",
        "                        # Implement if needed for this dataset\n",
        "                        continue\n",
        "\n",
        "                    # Handle polygon format\n",
        "                    elif \"polygon\" in obj:\n",
        "                        # Implement if needed\n",
        "                        continue\n",
        "\n",
        "            # Handle LabelMe format as alternative\n",
        "            elif \"shapes\" in anno_data:\n",
        "                for shape in anno_data[\"shapes\"]:\n",
        "                    label = shape.get(\"label\")\n",
        "                    if label not in class_to_id:\n",
        "                        continue\n",
        "\n",
        "                    class_id = class_to_id[label]\n",
        "                    points = shape.get(\"points\", [])\n",
        "\n",
        "                    if shape.get(\"shape_type\") == \"rectangle\" and len(points) == 2:\n",
        "                        # Rectangle format: [[x1,y1], [x2,y2]]\n",
        "                        x1, y1 = points[0]\n",
        "                        x2, y2 = points[1]\n",
        "\n",
        "                        # Calculate width and height\n",
        "                        x = min(x1, x2)\n",
        "                        y = min(y1, y2)\n",
        "                        w = abs(x2 - x1)\n",
        "                        h = abs(y2 - y1)\n",
        "\n",
        "                        # Convert to YOLO format\n",
        "                        x_center = (x + w/2) / img_width\n",
        "                        y_center = (y + h/2) / img_height\n",
        "                        w_norm = w / img_width\n",
        "                        h_norm = h / img_height\n",
        "\n",
        "                        yolo_annotations.append(f\"{class_id} {x_center} {y_center} {w_norm} {h_norm}\")\n",
        "\n",
        "                    elif shape.get(\"shape_type\") == \"polygon\" and len(points) > 2:\n",
        "                        # Get bounding box from polygon\n",
        "                        x_coords = [p[0] for p in points]\n",
        "                        y_coords = [p[1] for p in points]\n",
        "\n",
        "                        x1, y1 = min(x_coords), min(y_coords)\n",
        "                        x2, y2 = max(x_coords), max(y_coords)\n",
        "\n",
        "                        # Calculate width and height\n",
        "                        w = x2 - x1\n",
        "                        h = y2 - y1\n",
        "\n",
        "                        # Convert to YOLO format\n",
        "                        x_center = (x1 + w/2) / img_width\n",
        "                        y_center = (y1 + h/2) / img_height\n",
        "                        w_norm = w / img_width\n",
        "                        h_norm = h / img_height\n",
        "\n",
        "                        yolo_annotations.append(f\"{class_id} {x_center} {y_center} {w_norm} {h_norm}\")\n",
        "\n",
        "            # Save YOLO annotations to file\n",
        "            if yolo_annotations:\n",
        "                all_image_paths.append(img_path)\n",
        "\n",
        "                # Create label file with same name as image but .txt extension\n",
        "                label_filename = os.path.splitext(img_file)[0] + \".txt\"\n",
        "                label_path = os.path.join(output_base, \"labels\", label_filename)\n",
        "\n",
        "                # Write annotations to file\n",
        "                with open(label_path, 'w') as f:\n",
        "                    f.write('\\n'.join(yolo_annotations))\n",
        "\n",
        "                # Copy image to output directory\n",
        "                output_img_path = os.path.join(output_base, \"images\", img_file)\n",
        "                shutil.copy(img_path, output_img_path)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {anno_path}: {e}\")\n",
        "\n",
        "    print(f\"Successfully processed {len(all_image_paths)} images\")\n",
        "    return all_image_paths, output_base\n",
        "\n",
        "# Split data into train and validation sets\n",
        "def split_data(all_image_paths, output_base, train_ratio=0.8):\n",
        "    random.shuffle(all_image_paths)\n",
        "    split_idx = int(len(all_image_paths) * train_ratio)\n",
        "\n",
        "    train_images = all_image_paths[:split_idx]\n",
        "    val_images = all_image_paths[split_idx:]\n",
        "\n",
        "    print(f\"Split data: {len(train_images)} training images, {len(val_images)} validation images\")\n",
        "\n",
        "    # Create train/val directories\n",
        "    train_img_dir = os.path.join(output_base, \"train\", \"images\")\n",
        "    train_label_dir = os.path.join(output_base, \"train\", \"labels\")\n",
        "    val_img_dir = os.path.join(output_base, \"val\", \"images\")\n",
        "    val_label_dir = os.path.join(output_base, \"val\", \"labels\")\n",
        "\n",
        "    os.makedirs(train_img_dir, exist_ok=True)\n",
        "    os.makedirs(train_label_dir, exist_ok=True)\n",
        "    os.makedirs(val_img_dir, exist_ok=True)\n",
        "    os.makedirs(val_label_dir, exist_ok=True)\n",
        "\n",
        "    # Copy files to respective directories\n",
        "    for img_path in train_images:\n",
        "        img_filename = os.path.basename(img_path)\n",
        "        label_filename = os.path.splitext(img_filename)[0] + \".txt\"\n",
        "\n",
        "        # Copy image\n",
        "        shutil.copy(img_path, os.path.join(train_img_dir, img_filename))\n",
        "\n",
        "        # Copy label if it exists\n",
        "        src_label = os.path.join(output_base, \"labels\", label_filename)\n",
        "        if os.path.exists(src_label):\n",
        "            shutil.copy(src_label, os.path.join(train_label_dir, label_filename))\n",
        "\n",
        "    for img_path in val_images:\n",
        "        img_filename = os.path.basename(img_path)\n",
        "        label_filename = os.path.splitext(img_filename)[0] + \".txt\"\n",
        "\n",
        "        # Copy image\n",
        "        shutil.copy(img_path, os.path.join(val_img_dir, img_filename))\n",
        "\n",
        "        # Copy label if it exists\n",
        "        src_label = os.path.join(output_base, \"labels\", label_filename)\n",
        "        if os.path.exists(src_label):\n",
        "            shutil.copy(src_label, os.path.join(val_label_dir, label_filename))\n",
        "\n",
        "    return train_img_dir, val_img_dir\n",
        "\n",
        "# Create YAML configuration file for YOLOv8\n",
        "def create_yaml_config(output_base, classes):\n",
        "    config = {\n",
        "        'train': os.path.join(output_base, 'train', 'images'),\n",
        "        'val': os.path.join(output_base, 'val', 'images'),\n",
        "        'nc': len(classes),\n",
        "        'names': classes\n",
        "    }\n",
        "\n",
        "    yaml_path = os.path.join(output_base, 'dataset.yaml')\n",
        "    with open(yaml_path, 'w') as f:\n",
        "        yaml.dump(config, f, sort_keys=False)\n",
        "\n",
        "    print(f\"Created YOLOv8 config at {yaml_path}\")\n",
        "    return yaml_path\n",
        "\n",
        "# Main function to run the data preparation and training\n",
        "def main():\n",
        "    try:\n",
        "        # 1. Download dataset and explore directory structure\n",
        "        dataset_info = download_dataset()\n",
        "\n",
        "        # 2. Get class information\n",
        "        classes_data = get_class_info(dataset_info)\n",
        "\n",
        "        # Handle classes that are dictionaries\n",
        "        if classes_data and isinstance(classes_data, list):\n",
        "            if classes_data and isinstance(classes_data[0], dict):\n",
        "                # Extract the 'title' field as class names\n",
        "                if \"title\" in classes_data[0]:\n",
        "                    classes = [cls[\"title\"] for cls in classes_data]\n",
        "                elif \"name\" in classes_data[0]:\n",
        "                    classes = [cls[\"name\"] for cls in classes_data]\n",
        "                else:\n",
        "                    print(\"Warning: Could not determine class names from dictionaries.\")\n",
        "                    classes = [f\"class_{i}\" for i in range(len(classes_data))]\n",
        "            else:\n",
        "                classes = classes_data\n",
        "        else:\n",
        "            classes = classes_data\n",
        "\n",
        "        print(f\"Using classes: {classes}\")\n",
        "\n",
        "        if not classes:\n",
        "            print(\"Error: No classes found for the dataset.\")\n",
        "            return\n",
        "\n",
        "        # 3. Convert annotations to YOLO format\n",
        "        all_image_paths, output_base = convert_annotations(dataset_info, classes)\n",
        "\n",
        "        if not all_image_paths:\n",
        "            print(\"Error: No valid image-annotation pairs found.\")\n",
        "            return\n",
        "\n",
        "        # 4. Split data into train and validation sets\n",
        "        split_data(all_image_paths, output_base)\n",
        "\n",
        "        # 5. Create YAML configuration\n",
        "        yaml_path = create_yaml_config(output_base, classes)\n",
        "\n",
        "        # 6. Install YOLOv8 if not already installed\n",
        "        try:\n",
        "            import ultralytics\n",
        "            print(\"Ultralytics already installed\")\n",
        "        except ImportError:\n",
        "            print(\"Installing ultralytics...\")\n",
        "            !pip install -q ultralytics\n",
        "\n",
        "        # 7. Train YOLOv8 model\n",
        "        print(\"\\nSetup complete! Now you can train the model with:\")\n",
        "        print(\"from ultralytics import YOLO\")\n",
        "        print(\"model = YOLO('yolov8n.pt')  # Load a pretrained YOLOv8 model\")\n",
        "        print(f\"results = model.train(data='{yaml_path}', epochs=100, imgsz=640, batch=16, name='supermarket_model')\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()  # Print the full error traceback for debugging\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCkZ3bty8kBZ",
        "outputId": "c71dc65a-7fc9-4223-f6b2-6cc8da9bf6f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading supermarket shelves dataset...\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/humansintheloop/supermarket-shelves-dataset?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131M/131M [00:00<00:00, 159MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded to: /root/.cache/kagglehub/datasets/humansintheloop/supermarket-shelves-dataset/versions/2\n",
            "\n",
            "Exploring dataset directory structure:\n",
            "2/\n",
            "    Supermarket shelves/\n",
            "        (Found 1 JSON files)\n",
            "        Supermarket shelves/\n",
            "            annotations/\n",
            "                (Found 45 JSON files)\n",
            "            images/\n",
            "                (Found 45 image files)\n",
            "\n",
            "Found likely image directory: /root/.cache/kagglehub/datasets/humansintheloop/supermarket-shelves-dataset/versions/2/Supermarket shelves/Supermarket shelves/images (45 images)\n",
            "Found likely annotation directory: /root/.cache/kagglehub/datasets/humansintheloop/supermarket-shelves-dataset/versions/2/Supermarket shelves/Supermarket shelves/annotations (45 JSON files)\n",
            "Found meta.json at /root/.cache/kagglehub/datasets/humansintheloop/supermarket-shelves-dataset/versions/2/Supermarket shelves/meta.json\n",
            "Found 2 classes: ['Price', 'Product']\n",
            "Using classes: ['Price', 'Product']\n",
            "Using image directory: /root/.cache/kagglehub/datasets/humansintheloop/supermarket-shelves-dataset/versions/2/Supermarket shelves/Supermarket shelves/images\n",
            "Using annotation directory: /root/.cache/kagglehub/datasets/humansintheloop/supermarket-shelves-dataset/versions/2/Supermarket shelves/Supermarket shelves/annotations\n",
            "Converting annotations to YOLO format...\n",
            "Found 45 images and 45 annotation files\n",
            "Successfully matched 45 images with annotations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [00:00<00:00, 84.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed 45 images\n",
            "Split data: 36 training images, 9 validation images\n",
            "Created YOLOv8 config at yolo_dataset/dataset.yaml\n",
            "Installing ultralytics...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m122.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "Setup complete! Now you can train the model with:\n",
            "from ultralytics import YOLO\n",
            "model = YOLO('yolov8n.pt')  # Load a pretrained YOLOv8 model\n",
            "results = model.train(data='yolo_dataset/dataset.yaml', epochs=100, imgsz=640, batch=16, name='supermarket_model')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yaml\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. Check the current directory structure\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "yolo_dataset_dir = \"yolo_dataset\"\n",
        "\n",
        "# Check if directories exist\n",
        "train_dir = os.path.join(yolo_dataset_dir, \"train\", \"images\")\n",
        "val_dir = os.path.join(yolo_dataset_dir, \"val\", \"images\")\n",
        "yaml_path = os.path.join(yolo_dataset_dir, \"dataset.yaml\")\n",
        "\n",
        "print(f\"Train directory exists: {os.path.exists(train_dir)}\")\n",
        "print(f\"Validation directory exists: {os.path.exists(val_dir)}\")\n",
        "print(f\"YAML file exists: {os.path.exists(yaml_path)}\")\n",
        "\n",
        "# 2. Fix the YAML file to use absolute paths\n",
        "if os.path.exists(yaml_path):\n",
        "    with open(yaml_path, 'r') as f:\n",
        "        data = yaml.safe_load(f)\n",
        "\n",
        "    print(\"Current YAML content:\", data)\n",
        "\n",
        "    # Fix paths to be absolute\n",
        "    current_dir = os.getcwd()\n",
        "    data['train'] = os.path.abspath(train_dir)\n",
        "    data['val'] = os.path.abspath(val_dir)\n",
        "\n",
        "    # Write back the updated YAML\n",
        "    with open(yaml_path, 'w') as f:\n",
        "        yaml.dump(data, f, sort_keys=False)\n",
        "\n",
        "    print(\"Updated YAML content:\", data)\n",
        "    print(f\"Updated YAML saved to {yaml_path}\")\n",
        "\n",
        "# 3. Alternative: Create a new YAML file with correct paths\n",
        "new_yaml_path = \"dataset_fixed.yaml\"\n",
        "with open(new_yaml_path, 'w') as f:\n",
        "    yaml_data = {\n",
        "        'train': os.path.abspath(train_dir),\n",
        "        'val': os.path.abspath(val_dir),\n",
        "        'nc': 2,  # Number of classes, change if different\n",
        "        'names': ['Price', 'Product']  # Class names, update if different\n",
        "    }\n",
        "    yaml.dump(yaml_data, f, sort_keys=False)\n",
        "\n",
        "print(f\"Created new YAML file at {new_yaml_path} with absolute paths\")\n",
        "print(f\"Use this command to train:\\n\")\n",
        "print(f\"from ultralytics import YOLO\")\n",
        "print(f\"model = YOLO('yolov8n.pt')\")\n",
        "print(f\"results = model.train(data='{new_yaml_path}', epochs=100, imgsz=640, batch=16, name='supermarket_model')\")"
      ],
      "metadata": {
        "id": "BWmMejZk9QpP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6749273-7483-482b-a952-aa65e64d5859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content\n",
            "Train directory exists: True\n",
            "Validation directory exists: True\n",
            "YAML file exists: True\n",
            "Current YAML content: {'train': 'yolo_dataset/train/images', 'val': 'yolo_dataset/val/images', 'nc': 2, 'names': ['Price', 'Product']}\n",
            "Updated YAML content: {'train': '/content/yolo_dataset/train/images', 'val': '/content/yolo_dataset/val/images', 'nc': 2, 'names': ['Price', 'Product']}\n",
            "Updated YAML saved to yolo_dataset/dataset.yaml\n",
            "Created new YAML file at dataset_fixed.yaml with absolute paths\n",
            "Use this command to train:\n",
            "\n",
            "from ultralytics import YOLO\n",
            "model = YOLO('yolov8n.pt')\n",
            "results = model.train(data='dataset_fixed.yaml', epochs=100, imgsz=640, batch=16, name='supermarket_model')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "model = YOLO('yolov8n.pt')\n",
        "results = model.train(data='dataset_fixed.yaml', epochs=60, imgsz=640, batch=16, name='supermarket_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SvFdqwqAwZ1",
        "outputId": "2d8c9384-78f1-42c8-b43a-d37b2013dedc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6.25M/6.25M [00:00<00:00, 103MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.140 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset_fixed.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=60, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=supermarket_model, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/supermarket_model, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 755k/755k [00:00<00:00, 25.3MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,011,238 parameters, 3,011,222 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.35M/5.35M [00:00<00:00, 120MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 166.4±16.0 MB/s, size: 2873.7 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/yolo_dataset/train/labels... 36 images, 0 backgrounds, 1 corrupt: 100%|██████████| 36/36 [00:00<00:00, 250.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/yolo_dataset/train/images/005.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/yolo_dataset/train/images/009.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3022      1.3223      1.0841      1.0744      1.0623      1.0945      1.0893      1.1579      1.0835      1.2695      1.3175       1.113      1.3259       1.207       1.029      1.0884      1.3348      1.0344      1.3302      1.3182      1.3013        1.33      1.3374      1.0893      1.0893      1.0747\n",
            "      1.0867      1.2417      1.0791      1.3228      1.0311      1.3339      1.3348      1.1824      1.3214      1.3235      1.2944       1.205      1.1221      1.2463      1.1656      1.0813      1.3266      1.3296      1.0859        1.33      1.0919      1.1205      1.0911      1.0887      1.0792      1.3214\n",
            "      1.3395]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/yolo_dataset/train/images/018.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/yolo_dataset/train/images/022.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/yolo_dataset/train/images/031.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m/content/yolo_dataset/train/images/041.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/yolo_dataset/train/labels.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 104.8±26.9 MB/s, size: 3194.5 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/yolo_dataset/val/labels... 9 images, 0 backgrounds, 0 corrupt: 100%|██████████| 9/9 [00:00<00:00, 247.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/yolo_dataset/val/labels.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/detect/supermarket_model/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/supermarket_model\u001b[0m\n",
            "Starting training for 60 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/60      8.79G      2.545      3.916      1.668       1019        640: 100%|██████████| 3/3 [00:09<00:00,  3.02s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:05<00:00,  5.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129     0.0476     0.0413      0.026     0.0157\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/60      8.98G      2.636      3.889      1.688       1417        640: 100%|██████████| 3/3 [00:01<00:00,  1.65it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  3.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129     0.0602     0.0521     0.0333     0.0208\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/60      5.81G        2.4      3.805      1.518       1678        640: 100%|██████████| 3/3 [00:01<00:00,  2.06it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129     0.0836     0.0736     0.0484     0.0277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/60      9.79G      2.171      3.639      1.402       1627        640: 100%|██████████| 3/3 [00:01<00:00,  1.93it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  3.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129     0.0773     0.0668     0.0445     0.0228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/60       8.5G      2.061      3.334      1.322       1312        640: 100%|██████████| 3/3 [00:01<00:00,  2.31it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  3.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129     0.0473     0.0388     0.0297     0.0142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/60      7.57G      1.922      2.903      1.219        993        640: 100%|██████████| 3/3 [00:01<00:00,  1.93it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  3.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.057     0.0455     0.0519     0.0268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/60      7.76G      2.015       2.85      1.227       1170        640: 100%|██████████| 3/3 [00:01<00:00,  2.57it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.103     0.0921     0.0843     0.0386\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/60      6.78G      1.878      2.608      1.245        533        640: 100%|██████████| 3/3 [00:01<00:00,  2.10it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129     0.0982       0.11     0.0836     0.0361\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/60      6.81G          2      2.446      1.229        812        640: 100%|██████████| 3/3 [00:01<00:00,  2.73it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.111      0.134      0.109     0.0491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/60      6.81G      1.953      2.301       1.19        831        640: 100%|██████████| 3/3 [00:01<00:00,  2.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129       0.11      0.135      0.112     0.0526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      11/60      10.6G      2.259      2.187      1.188       2175        640: 100%|██████████| 3/3 [00:01<00:00,  2.07it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.679      0.129      0.121     0.0559\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      12/60      6.88G      1.911       2.04      1.099       1380        640: 100%|██████████| 3/3 [00:01<00:00,  2.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.718      0.107      0.144     0.0709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      13/60      9.38G      2.116      1.985      1.165       1285        640: 100%|██████████| 3/3 [00:01<00:00,  2.22it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  3.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.658      0.108      0.267      0.167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      14/60      5.41G      1.944      1.909      1.104       1672        640: 100%|██████████| 3/3 [00:01<00:00,  2.52it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.653      0.106      0.112     0.0526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      15/60      9.27G      2.036      1.754      1.113       1232        640: 100%|██████████| 3/3 [00:01<00:00,  1.70it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.673      0.125      0.113     0.0518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      16/60      7.93G      2.112      1.773      1.154       1316        640: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.673      0.135      0.117     0.0521\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      17/60      7.63G      1.794      1.687      1.068       1305        640: 100%|██████████| 3/3 [00:01<00:00,  2.44it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.683      0.138      0.125     0.0581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      18/60      10.4G      1.902      1.809      1.103       1409        640: 100%|██████████| 3/3 [00:01<00:00,  2.04it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.691      0.144      0.133     0.0603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      19/60       8.6G      1.822      1.747      1.078       1112        640: 100%|██████████| 3/3 [00:01<00:00,  1.92it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.679      0.146      0.131     0.0599\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      20/60       8.3G      1.814      1.703      1.113        522        640: 100%|██████████| 3/3 [00:01<00:00,  2.53it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.677      0.143      0.128     0.0578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      21/60       9.5G      1.854      1.632      1.112       1027        640: 100%|██████████| 3/3 [00:01<00:00,  2.44it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.671      0.136      0.116     0.0517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      22/60      5.81G      1.794      1.629      1.075       1307        640: 100%|██████████| 3/3 [00:01<00:00,  2.78it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.667      0.133      0.113     0.0505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      23/60       9.1G      1.902      1.494      1.083       1718        640: 100%|██████████| 3/3 [00:01<00:00,  1.87it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.661       0.13      0.118     0.0557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      24/60      8.48G      1.828      1.557      1.067       1057        640: 100%|██████████| 3/3 [00:01<00:00,  2.55it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.654      0.129      0.114     0.0533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      25/60      10.6G       1.79      1.554       1.11        597        640: 100%|██████████| 3/3 [00:01<00:00,  2.10it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.647       0.13       0.11     0.0498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      26/60      5.99G       1.82      1.549      1.159        443        640: 100%|██████████| 3/3 [00:01<00:00,  2.58it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.653      0.124      0.111     0.0515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      27/60      10.1G      1.853      1.643      1.074       1516        640: 100%|██████████| 3/3 [00:01<00:00,  1.99it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.657      0.131      0.114     0.0541\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      28/60      5.59G      1.758      1.432      1.054        826        640: 100%|██████████| 3/3 [00:01<00:00,  2.47it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.663       0.14      0.121     0.0559\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      29/60      9.59G      1.864      1.478      1.062       1446        640: 100%|██████████| 3/3 [00:01<00:00,  2.30it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.663       0.14      0.121     0.0559\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      30/60      8.61G      1.794      1.576       1.09        539        640: 100%|██████████| 3/3 [00:01<00:00,  2.32it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.656      0.146      0.127     0.0577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      31/60      6.89G       1.71       1.34      1.045       1319        640: 100%|██████████| 3/3 [00:01<00:00,  2.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.649      0.153      0.132     0.0582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      32/60      9.35G      1.692      1.426      1.044       1298        640: 100%|██████████| 3/3 [00:01<00:00,  2.36it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.661      0.144      0.138     0.0666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      33/60      8.68G      1.748      1.433      1.065       1344        640: 100%|██████████| 3/3 [00:01<00:00,  2.34it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.661      0.144      0.138     0.0666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      34/60      7.84G      1.927      1.444      1.067       2282        640: 100%|██████████| 3/3 [00:01<00:00,  1.95it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.176       0.19      0.165     0.0862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      35/60      6.76G      1.774      1.353      1.049       1360        640: 100%|██████████| 3/3 [00:01<00:00,  2.63it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.237      0.214      0.216      0.122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      36/60      10.7G      1.855      1.355      1.062       2141        640: 100%|██████████| 3/3 [00:01<00:00,  2.33it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.295      0.239      0.261      0.152\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      37/60      8.88G      1.664        1.3       1.03       1196        640: 100%|██████████| 3/3 [00:01<00:00,  2.39it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.295      0.239      0.261      0.152\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      38/60      8.23G      1.747      1.298      1.042       1468        640: 100%|██████████| 3/3 [00:01<00:00,  1.52it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.294      0.257      0.277      0.162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      39/60      6.38G      1.656      1.399      1.092        512        640: 100%|██████████| 3/3 [00:01<00:00,  2.58it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.318      0.278      0.298      0.172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      40/60       6.4G      1.783       1.38      1.052       1050        640: 100%|██████████| 3/3 [00:01<00:00,  2.70it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.318      0.299      0.309      0.171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      41/60        10G      1.734      1.427      1.094       1306        640: 100%|██████████| 3/3 [00:01<00:00,  2.78it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  3.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.318      0.299      0.309      0.171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      42/60      5.86G      1.696      1.259      1.021       1436        640: 100%|██████████| 3/3 [00:01<00:00,  1.97it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.326      0.319      0.324      0.173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      43/60      5.87G      1.685      1.278      1.027       1328        640: 100%|██████████| 3/3 [00:01<00:00,  2.52it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  3.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129       0.31      0.323      0.319       0.17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      44/60      10.2G      1.789      1.202      1.028       1802        640: 100%|██████████| 3/3 [00:01<00:00,  2.05it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.307      0.333      0.324      0.174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      45/60      9.55G      1.631      1.157      1.038        743        640: 100%|██████████| 3/3 [00:01<00:00,  2.40it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.307      0.333      0.324      0.174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      46/60      7.91G      1.763      1.295      1.041       1776        640: 100%|██████████| 3/3 [00:01<00:00,  1.88it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.311      0.344      0.328       0.18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      47/60      5.11G      1.611      1.144      1.003       1631        640: 100%|██████████| 3/3 [00:01<00:00,  2.49it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.303      0.339      0.323      0.176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      48/60      8.94G      1.818      1.234      1.019       2480        640: 100%|██████████| 3/3 [00:01<00:00,  2.40it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.309      0.339      0.327      0.172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      49/60      10.1G      1.739      1.208      1.031       1761        640: 100%|██████████| 3/3 [00:01<00:00,  2.30it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.309      0.339      0.327      0.172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      50/60      8.72G      1.609      1.129       1.01       1038        640: 100%|██████████| 3/3 [00:01<00:00,  1.98it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.447      0.327      0.333      0.173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      51/60      4.81G      1.648       1.33      1.042        492        640: 100%|██████████| 3/3 [00:13<00:00,  4.60s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.459      0.323      0.339      0.176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      52/60      6.36G      1.535      1.093      1.023        506        640: 100%|██████████| 3/3 [00:04<00:00,  1.57s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.479      0.319      0.347      0.182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      53/60      6.37G      1.528      1.094      1.029        398        640: 100%|██████████| 3/3 [00:03<00:00,  1.13s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.479      0.319      0.347      0.182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      54/60      6.39G       1.48      1.025     0.9928        757        640: 100%|██████████| 3/3 [00:00<00:00,  3.03it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  3.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.462      0.326      0.357       0.19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      55/60       6.4G      1.559      1.132      1.033        902        640: 100%|██████████| 3/3 [00:01<00:00,  1.62it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129       0.47      0.334      0.365      0.195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      56/60      6.43G      1.663      1.188      1.079        713        640: 100%|██████████| 3/3 [00:01<00:00,  2.61it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.466      0.334      0.369      0.198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      57/60      6.44G      1.778      1.284      1.051       1157        640: 100%|██████████| 3/3 [00:00<00:00,  3.17it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.466      0.334      0.369      0.198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      58/60      6.46G      1.559      1.008     0.9988        914        640: 100%|██████████| 3/3 [00:01<00:00,  2.04it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.455      0.338      0.375      0.203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      59/60      6.47G      1.492      1.118      1.038        629        640: 100%|██████████| 3/3 [00:01<00:00,  2.85it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.451      0.338      0.376      0.204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      60/60      6.49G      1.462      1.124      1.013        387        640: 100%|██████████| 3/3 [00:00<00:00,  3.03it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.445      0.344      0.378      0.205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "60 epochs completed in 0.061 hours.\n",
            "Optimizer stripped from runs/detect/supermarket_model/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from runs/detect/supermarket_model/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating runs/detect/supermarket_model/weights/best.pt...\n",
            "Ultralytics 8.3.140 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          9       2129      0.444      0.344      0.379      0.205\n",
            "                 Price          9        362      0.548      0.282      0.408      0.241\n",
            "               Product          9       1767      0.341      0.407      0.349      0.169\n",
            "Speed: 0.6ms preprocess, 4.6ms inference, 0.0ms loss, 7.5ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/supermarket_model\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the best model\n",
        "files.download('runs/detect/supermarket_model/weights/best.pt')\n",
        "\n",
        "# Optional: Also download the training results\n",
        "files.download('runs/detect/supermarket_model/results.csv')  # Training metrics\n",
        "files.download('runs/detect/supermarket_model/confusion_matrix.png')  # Confusion matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "HCSm2Wy0Bbe5",
        "outputId": "7c506dc7-9bec-4ec4-ac6d-ba77038cc9ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4f88df14-096c-4179-927d-bd9401e2b652\", \"best.pt\", 6249955)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4cc73005-6ea9-4acf-b090-5f9fa3d2a0a2\", \"results.csv\", 7716)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2d863f09-321c-43e4-a2c8-4bae61d4bb9f\", \"confusion_matrix.png\", 110547)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load your trained model\n",
        "model = YOLO('runs/detect/supermarket_model/weights/best.pt')\n",
        "\n",
        "# Export to ONNX format (for deployment)\n",
        "model.export(format='onnx')\n",
        "\n",
        "# Other formats available: 'torchscript', 'tflite', 'openvino', etc.\n",
        "# For example: model.export(format='tflite')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "v7OjLUjiDZZh",
        "outputId": "124f642a-9814-4819-e382-df86b5c0c3d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.140 🚀 Python-3.11.12 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n",
            "💡 ProTip: Export to OpenVINO format for best performance on Intel CPUs. Learn more at https://docs.ultralytics.com/integrations/openvino/\n",
            "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs/detect/supermarket_model/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (6.0 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['onnx>=1.12.0,<1.18.0', 'onnxslim>=0.1.53', 'onnxruntime-gpu'] not found, attempting AutoUpdate...\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 6.3s\n",
            "WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.53...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 7.7s, saved as 'runs/detect/supermarket_model/weights/best.onnx' (11.7 MB)\n",
            "\n",
            "Export complete (8.7s)\n",
            "Results saved to \u001b[1m/content/runs/detect/supermarket_model/weights\u001b[0m\n",
            "Predict:         yolo predict task=detect model=runs/detect/supermarket_model/weights/best.onnx imgsz=640  \n",
            "Validate:        yolo val task=detect model=runs/detect/supermarket_model/weights/best.onnx imgsz=640 data=dataset_fixed.yaml  \n",
            "Visualize:       https://netron.app\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'runs/detect/supermarket_model/weights/best.onnx'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wzntnmolECMo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}